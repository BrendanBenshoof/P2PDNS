% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{IEEEtran} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.



\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{url}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{algorithm} 
\usepackage{algorithmic}

% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy
%\renewcommand{\headrulewidth}{0pt} % customise the layout...
%\lhead{}\chead{}\rhead{}
%\lfoot{}\cfoot{\thepage}\rfoot{}

%%% END Article customizations

%%% The "real" document content comes below...

\title{The Distributed Dynamic Domain Name Service}
\author{
Brendan Benshoof \qquad Andrew Rosen  \\Department of Computer Science, Georgia State University\\ 34 Peachtree St NW \\ Atlanta, Georgia 30303\\  bbenshoof@cs.gsu.edu }
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\begin{abstract}
We have created a system to replace the top level of the Domain Name system. The goal is to replace the top level of DNS with a network of authoratative name servers sharing their DNS records over a DHT.

We attempt to address the core problem posed by centralized trusted third parties. Specifically that centralized authorities are a single point of failure for trust. We seek to diffuse the responsibility of these system such that abuse of trust is more difficult.

It is reverse compatible with traditional DNS and entirely transparent to end users.


\end{abstract}


\section{Introduction}
The Domain Name System, commonly referred to as DNS \cite{mockapetris2003rfc} \cite{mockapetris2004rfc}, is a fundamental component of the Internet.  DNs maps memorible names to the numerical IP addresses used by computers to communicate over IP. 

%cut this for conference?
Two recent events in the United states have brought DNS to the forfront of networking and security research.  First is recent legislation proposed in the US House of Representatives and US Senate. The Stop Online Piracy Act (SOPA) \cite{sopa} and PROTECT IP Act (PIPA) \cite{pipa} were both introduced in 2011.  There were numerous aspects to both bills, but essential to both was that DNS (non-authoratative?) servers located in the US would be required filter DNS records on demand, essentially fratcuring the DNS system.  There would be no guarantree that DNS could serve the same information to two different users.


More recent are the leaks of classified information elucidating the extent of the NSA's spying capabilities. These leaks have raised questions about the security of SSL and TLS, as well as the level of trust that users place in certificate authorities.

There have been many explorations and attempts\cite{cox}\cite{pappas}\cite{ramasubramanian2004design} to propose a DNS system based on a distributed hash table\cite{chord}. We extend on those papers by implementing a DHT which minimizes latency distance rather than hop distance and implementing a shared record of ownership based on recent developments in cryptocurency.\cite{bitcoin}\cite{namecoin}

These types of threats to DNS, along with security concerns, were not considered when designing the protocol, but DNS is too widely used and too integerated with the Internet as a whole to be replaced. Extensions such as DNSSEC \cite{blacka2013clarifications} add authentication and data integrity, but do not alter the fundenental architecture of the DNS network.


This paper proposes the Distributed Dynamic Domain Name Service, or D$^{3}$NS.  D$^{3}$NS is a completely decentralized Domain Name Service operating over a Distributed Hash Table (DHT).  D$^{3}$NS does not replace the DNS protocol, but rather adds robustness to the architecture as a whole.  Internally, D$^3$NS signs all DNS records using public/private keys.


We show that D$^{3}$NS satisfies the objections to a decentralized DNS system posed by Cox et al/cite{cox}, specificaly: dramatically reducing latency, retaining the extensibility of the orginal DNS system, and changing the intended scope of use to address incentive issues. We show D$^{3}$NS allows for new authentication methods and a means of decentralized proof of ownership beyond that of Cox et al's work. 



The rest of the paper is consists of the following sections.  Section gives an overview of DNS and identifies prior research in the area of distributed DNS.  Section covers the modified blockchain used for record authentication in D$^3$NS.  Section presents VHash, a DHT that we designed for D$^3$NS.  Section defines the various components of D$^3$NS, while Section details our implementation of D$^3$NS.  We discuss our conclusions and future work in Section.



\section{Background}



ITT we talk about DNS and work related to this paper.


This paper is intended to address concerns raised by Cox et al\cite{cox} and propose a viable decentralized DNS replacement based on a DHT. Our proposed improvements on the DNS alternative presented by Cox et al are full reverse compatibility with the current hierarchical DNS system and a shared authenticated public record that allows for DNSSEC style authentication.


\subsection{DNS Overview}

DNS has a heirarchical structure.

Knocking out certain servers can deny interent access to an entire region or zone.

SOPA \cite{sopa} is bad \cite{lemley2011don}. 

PROTECT IP is bad \cite{crocker2011security}

These bills would have required that servers maintained in the US filter specified domain names, preventing users from obtaining the correct IP address for the domain name in question. The 

This filtering is incompatable with the DNS Securtity Extensions (DNSSEC) \cite{crocker2011security}. 

The mandated dns filtering would drive users to unregulated DNS servers, which would create more attack vectors where users could have their traffic rredirected to a malicious website. 

\subsection{Related Work}
What were the concerns of the cox paper \cite{cox}?  
Higher latency because chord (plus none of the chord advantages). 
Wasn't extensible.  Served static records.  Really nothing more than a distributed text file
Not incentivised. No reason not be a defector in the system.

Advantages:  DDNS is more resistant to DDOS attacks.  Load balancing.


\section{Blockchain}
Our DNS records are kept via a block chain

\subsection{Blockchains in Bitcoin}
Bitcoin is a decentralized method of currency. Here we are particularly concerned with bitcoin's mechanism. Bitcoin's mechanism consists of a shared authenticateable transaction record.  \cite{bitcoin} \cite{namecoin}.

Bitcoin's blockchain is essentially a shared ledger.  The blockchain is a record of every single transaction made using the Bitcoin system. Each transation refers to previous transactions to indiacate the funds handled by a given transaction are in fact own by the user iniating the transactions. The record is validated by traversing the tree of transactions and marking referenced transactions as used. A valid blockchain has all non-leaf node transaction marked as used only once.

Each block in the chain is a series of transactions published during the time it takes to mine that block. A block is mined by generating a nonce field on the block chain such that the hash of the entire block is less then a difficulty value. This difficulty sets the rate at which new blocks are mined and it adjusted in reference to the number of miners. When a block is mined, it is transmitted to the network and each transaction in it is validated by each peer. The network then begins to mine the next block.


\subsection{Using the Blockchain to validate DNS records}
We utilize the transaction record of bitcoin to record ownership of domain names. The reward and incentive for mining a new block is a record that allots the miner the right to claim a domain name. The transactions in each block indicate miners claiming a new domain or the transfer of domain ownership. claims of new domains are validated by a reference to an unclaimed mining reward owned by the claiming user. Transfers are validated by a pointer to an unused pevious transfer record or claim record indicating ownership by the transfering party. This way every domain name in the system can be associated with an owner's private key. New domains can be claimed and old domains can be transfered between owners.

\subsection{Using a Blockchain to replace certificiate authority}
The shared record of the blockchain allows any participant in the mining network to act as a trusted third party to clients. This way trust is not centralized in single points of failure. Internally members of the DHT are also members of the blockchain network (as it is convient to use the DHT overlay as the Blockchain network overlay network) and thus all pushed records to the DHT and retrived records can be confirmed as legitimate before transmission to the end user. This limits the viability of replay or injection based attacks.


\begin{algorithm}
\caption{Blockchain mining}
\label{mining}
\begin{algorithmic}[1]  % the number is how many 
\STATE Given previous Block $B_{-1}$
\STATE Given New Transaction Set $T$
\STATE Given Difficulty $D$
\STATE Given Reward destionation $R$
\STATE New Block $B_0$ = $HASH(B_{-1})|T|R|Timestamp$
\STATE Block Attempt $b$ = $B_0|Nonce$
\WHILE {$HASH(b)<D$}
	\STATE Block Attempt $b$ = $B_0|Nonce$
\ENDWHILE
\STATE Propogate $b$ as next block
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Blockchain Transaction Validation}
\label{mining}
\begin{algorithmic}[1]  % the number is how many 
\STATE A new transaction $t$ consits of: Award domain $D$ to user $U$ with proof reference $P$ and signature $S$
\STATE The transaction set $T$ is the set of all transactions considered valid
\IF{$P$ is not marked used}
	\IF{owner indicated in $P$ matches signature $S$}
		\IF{$D$ matches domain referenced in $P$}
    		\STATE Mark $P$ as used
            \STATE Consider $t$ valid
        \ELSE
         	\IF{$P$ is a mining reward}
            	\STATE $P$ is an unclaimed mining reward
                \IF{$D$ is not yet claimed}
                	\STATE Mark $P$ as used
                    \STATE Mark $D$ as claimed
                    \STATE Consider $t$ valid
                \ENDIF
			\ENDIF
         \ENDIF
    \ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}




\subsection{Unadressed Security Issues}
The Blockchain does not solve all security issues relavent to DNS authentication and security. Exit nodes could lie or have packets inject to clients until the protocol from DNS server to client is improved. The DHT structure opens up unexplored disruption attacks on the overlay topology.



\section{VHash}
VHash was created to allow for spacial respresentations to be mapped to hash locations, a feature lacking in many current distributed hash tables.  In particular, we aimed to construct a mechanism for creating a more efficient global scale DHT built on a minimal latency overlay. Rather than focus on minimizing the amount of hops required to travel from point to point we wish to minimize the time required for a message to reach its recepient. VHash actually has a worse worst case hop distance ($O(n^{\frac{1}{d}})$) then other comparable Distributed Hash Tables ($O(lg(n))$) however VHash can route messages as quickly as possible rather than a grand tour an overlay network may describe in the real world.

The naive method of doing so is to assign coordinates to servers based on embedding location of nodes. More complex approaches would approximate a minimum latency space based on inter-node latency. VHash can be considered  a generallized extension of VoroNet \cite{voronet}.

%The intent is that meaning can be ascribed to locations in the DHT and facilitate more efficent function. 
%Specifically in this example we seek to build a minimal latency overlay network for the DHT so a global scale DHT is viable and efficent. 


\begin{algorithm}
\caption{Vhash Maintence Cycle}
\label{maintence}
\begin{algorithmic}[1]  % the number is how many 
	\STATE $P_0$ is this node's set of peers
    \STATE $T$ is the maintence period
    \WHILE{Node is running}
    	\FORALL{node $n$ in $P_0$}
        	\STATE Send a Maintence Message containing $P_0$ to $n$
        \ENDFOR
    \STATE Wait $T$ seconds
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Vhash Handle Maintence Message}
\label{handelmaintence}
\begin{algorithmic}[1]  % the number is how many 
	\STATE $P_0$ is this node's set of peers
	\STATE Recive a Maintence Message from peer $n$ containg its set of peers:$P_n$
    \FORALL{Peers $p$ in $P_n$}
    	\STATE Consider $p$ as a member of $P_0$
        \IF{$p$ should join $P_0$}
        	\STATE Add $p$ to $P_0$
            \FORALL {Other peers $i$ in $p$}
            	\IF{$i$ is occuluded by $p$}
                	\STATE remove $i$ from $P_0$
                \ENDIF
            \ENDFOR
        \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Vhash Join}
\label{routing}
\begin{algorithmic}[1]  % the number is how many 
\STATE new node $N$ wishes to join and has location $L$
\STATE $N$ knows node $x$ to be a member of the network
\STATE $N$ sends a request to join, addressed to $L$ via $x$
\STATE node $Parent$ is responsible for location $L$ and recives the join message
\STATE $Parent$ sends to $N$ its own location and list of peers
\STATE $Parent$ integrates $N$ into its peer set
\STATE $N$ builds its peer list from $N$ and its peers
\STATE regular maintence updates other peers
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Vhash Routing}
\label{routing}
\begin{algorithmic}[1]  % the number is how many 
	\STATE $P_0$ is this node's set of peers
    \STATE $N$ is this node
	\STATE $m$ is a message addresed for $L$
    \STATE $Forwards$ is the set $P_0\cup{}N$
    \STATE find $C$: member of $Forwards$ which has the shortest distance to $L$
    \IF{$C$ is $N$}
    	\STATE $N$ is the responsible party.
        \STATE Handle $m$
    \ELSE
    	\STATE Forward $m$ to $C$ for handleing or further routing
    \ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Toroidal Distance Equation}

Given two vector locations $\vec{a}$ and $\vec{b}$ on a  $d$ dimensional unit cube
\[ distance = \sqrt[|d|]{\sum\limits_{i\in d} (\min(|\vec{a}_i-\vec{b}_i|,1.0-|\vec{a}_i-\vec{b}_i|))^2}\]

\subsection{Mechanism}
VHash maps nodes to an overlay arbitrary $d$ dimension toroidal unit space. This is essentially a hypercube with wrapping edges. The toroidal property makes some visualization and mathmatical properties difficult \footnote{which one} but allows for a space without a sparse edge  \footnote{a picture here would help demonstrate this}.  The lack of a edge due to wrapping allows higher dimension spaces to be embedded with less error \footnote{I assume we prove this somewhere}.

VHash nodes are responsible for the address space defined by their Voronoi region. This region is definded by a list of peer nodes maintained by the node. A minimum list of peers is maintained such that the node's Voronoi region is well definded. The links connecting the node to its peers correspond to the links of a Delunay Triangulation. 

\subsection{Relation to Voronoi Diagrams and Delunay Triangulation}

VHash does not strictly solve Voronoi diagrams \cite{voronoi}. The toroidal nature of the space preclude the traditional means of solving Voronoi regions. However, VHash's peer management approximates a graph with similar properties. 
%Rather than attempt to calculate the Voronoi region of each node, it simply filters locations, assigning responsibility to the nearest node. 
An online algorithim maintains the set of peers defining the node's voronoi region. The set of peers required to define a node's voronoi region descibe a solution to the dual Delunay Triangulation.\footnote{this seems to be a bit repitative}

\subsection{Messages}
Maintence and joining are handled by a simple periodic mechanism. A notification message consisting of a node's information and active peers is the only maintence message. All messages have a destination hash location which is used to route them to the proper server. This destination can be the hash location of a particular node or the location of a desired record or service.  The message is recived by the node responsible for the location. Services running on the DHT define their own message contents.

\subsection{Message Routing}
Messages are routed over the overlay network using a simple algorithim. A node maintains a minimal list of peers to define it's own voronoi region.  From the perspective of the node, the entire voronoi region is defined only by its peers.

When routing a message to an arbitrary location, a node calculates who's voronoi region the message's destination is in amoungst the node and its peers. If the destinion falls within its own region then it is responsible and handles the message acordingly. The node otherwise forwards the message to the closest peer to the destination location. This process describes a pre-computed and cached A* \cite{astar} routing algorithim.

\subsection{Joining and Maintence}
Joining the network is a straightforward process. A new node first learns the location of at least one member of the network to join. The joining node then choses a location in the hash space either at random or based on a promblem formulation (for example, based on geographic location or latency information).

After choosing a location, the joining node sends a "join" message to its own location via the known node.
The message is forwarded to the current owner of that location who can be considered a "parent" node.
The parent node imediately replies with a maintenence message containing its full peer list. This message makes its way to the joining node, who then uses this to begin defining the space he is responsible for. 

The joining nodes final peers are a subset of the parent and the parent's peers. The parent adds the new node to it's peer list and removes all his peers occluded by the new node.  Then regular maintence propogates the new node's information and repairs the overlay topology.

Each node in the network performs maintenence periodically by sending nodes a maintenence message. The maintence message consists of the node's information and the information on that node's peer list. When a maintence message is recived, the reciving node considers the listed nodes as candidates for its own peer list and removes any occluded nodes from their own peer list. 

When messages sent to a peer fail, it is assumed the peer has left the network. The leaving peer is removed from the peer list and canidates from the set of 2-hop peers provided by other peers move in to replace it.

There is no protocol for a "polite" exit from the network. The DHT protocol assumes nodes will fail and the diference between an intended failure and unintended failure is unecessary. The only issue this causes is that software should be designed to fail totaly when issues arrise rather then attempt to fulfill only part of the protocol.  

\subsection{Data Storage and Backups}
The primary goal of a DHT is to provide a distributed storage medium. We extend this idea  to distribute work and information among nodes using the same paradigm. Resources in the network, be it raw data or assigned tasks, are assigned hash locations. The node responsible for a given hash location is responsible for the maintence of that resource. When a node fails, its peers take responsibility of its space. Thus it is important to provide peers with frequent backups of a node's assigned resources.  That way, when a node fails, its peers can immediately assume its responsibilities.

When a resource is to be stored on the network, it is assigned a hash location. The hash locations assigned could be random, a hash of an identifier, or have specific meaning for an embedding problem. The node responsible for that resaource's hash location stores the resource.

A resource is accessed by contacting the node responsible for the resource.  However, the requester generally has no idea which node is responsible for any particular resource.  The data request messsage is addressed to the location corresponding to the resource, rather than the node responsible for that location.  The message is forwarded over the overlay network, each hop bringong the node closer until it reaches the responsible node, who sends the resource or an error if the resource does not exist.

Some options are imediately apparent for dealing with wasted storage space. If usage of the system is temporary then accept the loss of efficency \footnote{If the data is intended to be stored only temporarily, then there is an inevitable hit to efficiency.}.  A system that is primarily read driven can record the time of the last read or a frequency of reads such that resources that are not read often enough after a certain period of time are deleted. If a system is write driven, allow the resource to be assigned a time to live, which can be updated as needed.

\subsection{Backups}
At a frequency set by the node manager or at a frequency set by any adaptive process to optimize backup frequency, a node send a message containing backups of the resources for which it is newly responsible to each of its peers. To minimize bandwidth and time wasted by backups, the node should only send the records changed since last backup

\subsection{Unaddressed Issues}
A series of issues relevent to VHash are not addressed in this paper due to limiting of scope, space, time, and a lack of actual solutions to the problems: The exact method of caching to optimize lookup time under real world usage. The overlay network being mapped onto latency results in nodes whose failure due to natural disaster to be comorbid with it's neighbors resulting to data loss. Comorbidness could be counteracted by more complex backup schemes.

\section{D$^{3}$NS}
D$^{3}$NS has logically descrete components which provide DNS efficent record storage, Domain name ownership management and verification, DNS backwards compatitibility, all of which may be logically replaced or have individual optimizations. D$^{3}$NS uses a DHT to store DNS records in a distributed fashion, A blockchain and Namecoin\cite{namecoin} analog to manage domain name ownership.

D$^{3}$NS utilizes public and private key encryption for signing and verifying records.


\subsection{Distributed Hash Table}
Our implementation is not specific to any particular Distributed Hash Table.  We examined using Chord \cite{chord} with DNS, similar to DDNS \cite{cox}.  However, Chord’s unidirectional ring overlay topology does not take actual network topology into account and using it for a global scale system is not viable because messages will be routed very inefficiently. A DHT which allows the routing overlay to be optimized to the network topology and conditions in real time is required.

As a result we chose to develop a prototype DHT to meet this requirement to act as backend to our DNS system called Vhash. Vhash is built on the idea of an “overlay space” which is a k-dimensional unit cube which wraps around the edges in a toroidal fashion. Each record in the DHT is assigned a location in that space. Each node is assigned a location and is responsible for records to which it is the closest node. The variable dimensionality is allowed so that problems can be embedded into the space with relative ease and records can be assigned locations which have meaning concerning the problem in which they are a part. This way, records that are close to each other in the problem formulation are close to each other in the DHT and are likely hosted on the same node. This offers speedup for many distributed algorithms which require traversal of data.



\subsection{DNS frontend}
Because this system is intended to be reverse compatible with the existing DNS protocol, we serve the data provided by the DHT after it has been authenticated by the block chain to other DNS servers or clients. DNS nodes incorporated into the D$^3$NS system will not request data from other DNS servers and will only exchange data via the DHT 

\section{Implementation}
Im not sure what to put here, this indicates it needs to be re-organized
\subsection{Establishment of a New domain}
Under the current DNS system, a new domain name is purchased from a company registered with the Internet Corporation for Assigned Names and Numbers (ICANN). That company adds the domain name and a record provided by the owner to the TLD servers. The owner or management company then maintains a name server to answer DNS requests for the purchased domain. In D$^{3}$NS new domain names are instead awarded as part of the blockchain mining process or purchased from a previous owner, then transferred to the new owner. These assignments and transfers are both recorded in the blockchain. 

A prospective domain owner can create their own mining software and mine a domain name or purchase a domain name voucher from a miner and exchange it for a domain name. In the blockchain, domain name owners are referred to by their public key which is used to authenticate records and transfer domains. Loss of the private key of an account will result in the loss of ability to update DNS records and ability to transfer the domain. 

\subsection{Updating records for a domain}
A domain name record in the current DNS system is used to configure a record on your own Name Server or to configure the record held by the TLD server to contains an address record. Using P2PDDNS all records must be signed using their owner's private key. 

A properly configured P2PDNS server should not accept a DNS records which has not been signed by its blockchain confirmed owner or accept a record with an older version number. To push a new DNS record for a domain the owner must create the record set for the domain and then sign and submit it to a node on the DHT. The DHT will forward the record to the responsible party and store it after confirming its validation. The new record will begin to be broadcast to clients after old records begin to expire and caches seek new records.

\subsection{Looking up a DNS record}
In the current DNS a record is looked up using a UDP system that queries a tree of requests. clients send queries to a local DNS server which acts as a resolver and cache. If a portion of the domain name is unknown the resolver sends a request to the responsible server and looks up recursively from there.

This system is largely unchanged under D$^{3}$NS from the point of view of the resolver and client. Ideally the resolver or client has chosen a nearby member of the D$^{3}$NS network as its root domain server (it can also maintain a large list of backup servers should its current one fail). The resolver requests a domain’s record from the D$^{3}$NS node. The node then forwards the request to the responsible party. If any node along the route has a valid cache of the required record, then that server responds and routes the message back to the entry node.All nodes along the route cache the response to aid future queries.

\subsection{Caching}
DHT(DNS?) needs to aggressively cache lookups. Previous investigations into optimizing caching on a DHT saw good results \cite{irm} however with the sepcific application of DNS in mind the time to live field on DNS records deffers the caching optimization problem onto users who we may or may not trust.

Integrated File Replication and Consistancy Maintanence (IRM) \cite{irm} views the process of caching and keeping the cache up to date as components of larger problem.  Nodes in the DHT keep track of how often records are requested and cache those records once a defined rate is passed.  Nodes then request an update for the cache based on how often the record is requested and how often that request is expected to be changed by the owner.


In the current implementation of DNS, (who does it) caching involves a time to live field defined by the domain owner. This means that there's not reall any sort of cache optimization done by the server; rather the server that thew records have a sensible time-to-live value.



IRM \cite{irm}  can be used to approximate the correct time-to-live value.


\subsection{The Big Picture}
Using all of these components together allows us to create a system with the following features:
\begin{itemize}
	\item Robustness - The DHT and Blockchain are both robust to failures and attacks
	\item Extensibility - The DNS reverse compatibility allows any DNS extension to be utilized, if dynamic resolution is required a name server record can be stored in the DHT to point to a user's specialized DNS servers.
	\item Decentralization - Both the DHT and Blockchain can operate without the support of any controlling organization, this offers security against corruption and abuse.
\end{itemize}	


\section{Practical Demonstration}
D$^3$NS was successfully  deonstrated.  We did it in class



\section{Conclusion and Future Work}
\bibliographystyle{plain}
\bibliography{P3DNS}
\end{document}
